---
title: "Scrape_Abgeordnete_Questions"
format: revealjs
editor: visual
---

# Questions to Abgeordnete

In this script questions and meta-information like date, tag, parliament etc. will be scraped and added to the main dataset.

## Load packages and data

```{r}
library(tidyverse)
library(rvest)
library(uuid)
politicians <- read.csv("politicians_v3.csv")
```

## Script to scrape questions and meta-infos

```{r}
profiles <- politicians$Q_A_Link

questions_meta_data <- data.frame(
  Date_q = character(),
  Question_text = character(),
  Question_teaser = character(),
  Question_link = character(),
  Answer_text = character(),
  qID = character(),
  Topic = character(),
  Parliament = character()
)

## TODO: Es muss noch über Anzahl der Seiten bei den Frage iteriert werden

## TODO: Es muss noch über die einzelnen Politiker iteriert werden


html <- read_html(profiles[1])

# extract date of questions
date_q <- html_elements(html, "div > div.tile__entry:first-child > div > div.tile__politician__info > span.tile__politician__label") %>%
  html_text()  %>%
  str_split("•") 
date_q <- c(date_q[[1]][2],date_q[[2]][2], date_q[[3]][2], date_q[[4]][2], date_q[[5]][2], date_q[[6]][2]) %>%
  trimws("l")

# extract teaser of questions as list
teaser_q <- html_elements(html, "div.tile__question__teaser > a") %>%
  html_text()

# extract teaser of answers as list
teaser_a <- html_elements(html, "div.tile__answer__summary > p") %>%
  html_text()

# generate unique id for each question
qID <- c(UUIDgenerate(), UUIDgenerate(), UUIDgenerate(), UUIDgenerate(), UUIDgenerate(), UUIDgenerate())

# extract topic 
topic <- html_elements(html, "div > ul.list-inline > li:last-child > a") %>%
  html_text()

# extract parliament where question was posed
parliament <- html_elements(html, "div > ul.list-inline > li:first-child > a") %>%
  html_text()

# extract link from question teasers as list
link_q <- html_elements(html, "div.tile__question__teaser > a") %>%
  html_attr("href")

# set link pieces together
question_page <- paste("https://www.abgeordnetenwatch.de", link_q, sep = "")

question_text <- vector()
answer_text <- vector()

for (e in 1:length(question_page)) {
  # scrape question and answer from the questionpage
  question_html <- read_html(question_page[e])
  
  # extract the question text
  text_q <- html_elements(question_html, "div.tile__question-text > div") %>%
    html_text()
  
  
  # if text_q has 0 characters the teaser is the question
  if (length(text_q) == 0) {
    text_q <- NA
  }
  
  question_text <- append(question_text, text_q)

  
  # extract the answer text
  text_a <- html_elements(question_html, "div.question-answer__text > div") %>%
    html_text()
  
  if (length(text_a) == 0) {
    text_a <- NA
  }
  
  answer_text <- append(answer_text, text_a)
}

questions_meta_data <- rbind(questions_meta_data, list(date_q, question_text, teaser_q, answer_text, qID, topic, parliament))
```
