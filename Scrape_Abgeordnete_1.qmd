---
title: "data_scraping"
format: html
editor: visual
---

## Load packages

```{r}

library("tidyverse")

```

## Load infos of politicians from Abgeordnetenwatch.de

This part will scrape the name, party and parliament from the website.

Known issues:

-   Some politicians will appear multiple times in the dataframe even though they are in just one parliament

-   Some links to the questions and answers are broken, because the links of the profile and the questions and answers do not have the same base structure

```{r}
# get number of pages
html <- read_html("https://www.abgeordnetenwatch.de/profile?parliament_period=All&party=All&page=0")
page <- html_element(html, "div.l-grid__col > nav ul > li:last-child > a") %>%
    html_attr("href") %>%
    str_split("=")
page <- as.numeric(page[[1]][4])
   
page_num <- c(0:page)
  
# starting url
url <- "https://www.abgeordnetenwatch.de/profile?parliament_period=All&party=All&page="

politicians <- data.frame(
  Name = character(),
  Party = character(),
  Parliament = character(),
  Profile_link = character()
)
  
for (i in page_num)  {
  # create link for each page
  updated_url <- paste(url, as.character(i), sep = "")
  
  # download the html of the given url
  html <- read_html(updated_url)
  
  # get node which contains the name
  node_name <- html_elements(html, ".tile__politician__name")
  # init empty name vector to store names from for loop
  names <- vector()
  # extract name
  for (e in 1:length(node_name)) {
    name <- html_text(node_name[[e]])
    names <- append(names, name)
  }

  # get node which contains the party
  node_party <- html_elements(html, ".tile__politician__party")
  # init empty vector to store party from for loop
  parties <- vector()
  # extract party
  for (e in 1:length(node_party)) {
    party <- html_text(node_party[[e]])
    parties <- append(parties, party)
  }
  
    # get node which contains the parliament
  node_parliament <- html_elements(html, "div.tile__politician-detail__candidacy-mandate:first-child > .tile__politician-detail__candidacy-mandate-label")
  # init empty vector to store parliament from for loop
  parliaments <- vector()
  # extract parliament
  for (i in 1:length(node_parliament)) {
    parliament <- html_text(node_parliament[[i]])
    # remove 'abegordnete:r'
    parliament <- unlist(strsplit(parliament, " ", fixed = TRUE))[2]
    parliaments <- append(parliaments, parliament)
  }
  
  # init empty vector
  links <- vector()
  # get nodes from html containig links
  node_link <- html_elements(html, ".link-gray")
  
  # extract link info and create full link
  for (i in 1:length(node_link)) {
    profile <- html_attr(node_link[[i]], "href")
    link_start <- "https://www.abgeordnetenwatch.de/"
    link_end <- "/fragen-antworten?page=0"
    # add links together
    link <- paste(link_start, profile, link_end, sep = "")
    
    links <- append(links, link)
  }
  
  # add data into dataframe
  politicians <- rbind(politicians, list(names, parties, parliaments, links))
}
```

## Export scraped data

```{r}
# export the politicians as csv
# the path has to be updated
write.csv(politicians, "Your\\file\\path\\politicians_1.csv", row.names = FALSE)
```
